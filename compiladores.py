# -*- coding: utf-8 -*-
"""Compiladores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZKdb19TcJjKqQE0i4WVVWgtrx9Nm1N8n
"""

# trafficlang_compiler.py
# Compilador TrafficLang - Unidade II
# Inclui: Lexer, Parser simplificado, SemanticAnalyzer, CodeGenerator e Simulator
# Atualizado para ignorar comentários iniciados por '#'

from enum import Enum, auto
import json
from dataclasses import dataclass
from typing import List, Dict, Any, Optional


# =========================================================
# TOKENS / LEXER
# =========================================================
class TokenType(Enum):
    IDENTIFIER = auto()
    NUMBER = auto()
    STRING = auto()
    KEYWORD = auto()
    OPERATOR = auto()
    ASSIGN = auto()
    SEMICOLON = auto()
    LPAREN = auto()
    RPAREN = auto()
    LBRACE = auto()
    RBRACE = auto()
    EOF = auto()


class Token:
    def __init__(self, token_type: TokenType, value: str, line: int, column: int):
        self.type = token_type
        self.value = value
        self.line = line
        self.column = column

    def __repr__(self):
        return f"Token({self.type.name}, '{self.value}', L{self.line}:C{self.column})"


class Lexer:
    def __init__(self, source_code: str):
        self.source = source_code
        self.position = 0
        self.line = 1
        self.column = 0
        self.keywords = {"sensor", "semaforo", "rota", "veiculo", "if", "else", "then", "end", "true", "false"}

    def next_char(self) -> Optional[str]:
        if self.position >= len(self.source):
            return None
        ch = self.source[self.position]
        self.position += 1
        if ch == '\n':
            self.line += 1
            self.column = 0
        else:
            self.column += 1
        return ch

    def peek(self) -> Optional[str]:
        return self.source[self.position] if self.position < len(self.source) else None

    def tokenize(self) -> List[Token]:
        tokens: List[Token] = []
        while True:
            ch = self.next_char()
            if ch is None:
                tokens.append(Token(TokenType.EOF, "", self.line, self.column))
                break

            # Ignorar espaços e tabulações
            if ch in " \t\r":
                continue

            # Ignorar comentários (# até o fim da linha)
            if ch == "#":
                while self.peek() not in (None, "\n"):
                    self.next_char()
                continue

            # Ignorar quebras de linha
            if ch == "\n":
                continue

            # Identificadores e palavras-chave
            if ch.isalpha() or ch == '_':
                start_col = self.column
                ident = ch
                while self.peek() and (self.peek().isalnum() or self.peek() == '_'):
                    ident += self.next_char()
                token_type = TokenType.KEYWORD if ident in self.keywords else TokenType.IDENTIFIER
                tokens.append(Token(token_type, ident, self.line, start_col))
                continue

            # Números
            if ch.isdigit():
                start_col = self.column
                num = ch
                while self.peek() and self.peek().isdigit():
                    num += self.next_char()
                tokens.append(Token(TokenType.NUMBER, num, self.line, start_col))
                continue

            # Strings
            if ch == '"':
                start_col = self.column
                s = ""
                while self.peek() and self.peek() != '"':
                    s += self.next_char()
                if self.peek() == '"':
                    self.next_char()
                tokens.append(Token(TokenType.STRING, s, self.line, start_col))
                continue

            # Atribuição
            if ch == '=':
                tokens.append(Token(TokenType.ASSIGN, ch, self.line, self.column))
                continue

            # Pontuação e operadores
            if ch == ';':
                tokens.append(Token(TokenType.SEMICOLON, ch, self.line, self.column))
                continue

            if ch in "{}()":
                mapping = {'{': TokenType.LBRACE, '}': TokenType.RBRACE, '(': TokenType.LPAREN, ')': TokenType.RPAREN}
                tokens.append(Token(mapping[ch], ch, self.line, self.column))
                continue

            if ch in "<>!":
                start_col = self.column
                nxt = self.peek()
                op = ch
                if nxt == '=':
                    op += self.next_char()
                tokens.append(Token(TokenType.OPERATOR, op, self.line, start_col))
                continue

            if ch in "+-*/":
                tokens.append(Token(TokenType.OPERATOR, ch, self.line, self.column))
                continue

            # Qualquer outro caractere
            tokens.append(Token(TokenType.OPERATOR, ch, self.line, self.column))

        return tokens


# =========================================================
# PARSER SIMPLIFICADO
# =========================================================
class Parser:
    def __init__(self, tokens: List[Token]):
        self.tokens = tokens
        self.pos = 0

    def peek(self) -> Token:
        """Olha o próximo token sem consumir."""
        if self.pos < len(self.tokens):
            return self.tokens[self.pos]
        return Token(TokenType.EOF, "", -1, -1)

    def next(self) -> Token:
        """Retorna o token atual e avança."""
        if self.pos < len(self.tokens):
            tok = self.tokens[self.pos]
            self.pos += 1
            return tok
        return Token(TokenType.EOF, "", -1, -1)

    def parse(self) -> List[Dict[str, Any]]:
        """Cria a AST a partir da lista de tokens."""
        ast_nodes = []
        while True:
            tok = self.peek()

            # Fim do código
            if tok.type == TokenType.EOF:
                break

            # ----------------------------------------------------
            # DECLARAÇÕES
            # Ex: sensor s1;
            # ----------------------------------------------------
            if tok.type == TokenType.KEYWORD and tok.value in ("sensor", "semaforo", "rota", "veiculo"):
                typ = self.next().value
                name_tok = self.next()

                # Confere se o próximo é ponto e vírgula
                semicolon = self.next()
                if semicolon.type != TokenType.SEMICOLON:
                    raise Exception(f"Erro de parsing: ';' esperado em L{semicolon.line}:C{semicolon.column}")

                ast_nodes.append({
                    "type": "declaration",
                    "name": name_tok.value,
                    "symbol_type": typ
                })
                continue

            # ----------------------------------------------------
            # ATRIBUIÇÕES
            # Ex: s1 = 5;
            # ----------------------------------------------------
            if tok.type == TokenType.IDENTIFIER:
                name_tok = self.next()
                next_tok = self.peek()

                # Se não houver '=', apenas ignora a linha (tolerância)
                if next_tok.type != TokenType.ASSIGN:
                    # Ignorar até o ponto e vírgula
                    while self.peek().type not in (TokenType.SEMICOLON, TokenType.EOF):
                        self.next()
                    if self.peek().type == TokenType.SEMICOLON:
                        self.next()
                    continue

                # Consome '='
                self.next()
                val_tok = self.next()
                val = val_tok.value

                # Confere ';'
                term = self.next()
                if term.type != TokenType.SEMICOLON:
                    raise Exception(f"Erro de parsing: ';' esperado em L{term.line}:C{term.column}")

                ast_nodes.append({
                    "type": "assignment",
                    "name": name_tok.value,
                    "value": val
                })
                continue

            # ----------------------------------------------------
            # CONDIÇÕES SIMPLIFICADAS
            # Ex:
            # if sensA > 10 then
            #   R1 = 1;
            # end
            # ----------------------------------------------------
            if tok.type == TokenType.KEYWORD and tok.value == "if":
                if_tok = self.next()
                left_tok = self.next()
                op_tok = self.next()
                right_tok = self.next()
                then_tok = self.next()

                if then_tok.value != "then":
                    raise Exception(f"Erro de parsing: 'then' esperado em L{then_tok.line}")

                # Ação dentro do if
                act_name = self.next()
                assign_tok = self.next()
                if assign_tok.type != TokenType.ASSIGN:
                    raise Exception(f"Erro de parsing: '=' esperado em L{assign_tok.line}:C{assign_tok.column}")

                act_val = self.next()

                # Opcionalmente consumir ';'
                if self.peek().type == TokenType.SEMICOLON:
                    self.next()

                # Confere se termina com 'end'
                end_tok = self.next()
                if end_tok.value != "end":
                    raise Exception(f"Erro de parsing: 'end' esperado em L{end_tok.line}")

                ast_nodes.append({
                    "type": "condition",
                    "condition": {
                        "left": left_tok.value,
                        "op": op_tok.value,
                        "right": right_tok.value
                    },
                    "action": {
                        "type": "assignment",
                        "name": act_name.value,
                        "value": act_val.value
                    }
                })
                continue

            # ----------------------------------------------------
            # Se não reconheceu nada, consome e segue
            # ----------------------------------------------------
            self.next()

        return ast_nodes


# =========================================================
# SEMÂNTICA / TABELA DE SÍMBOLOS
# =========================================================
class Symbol:
    def __init__(self, name, typ, value=None):
        self.name = name
        self.type = typ
        self.value = value

    def __repr__(self):
        return f"{self.name}: {self.type} = {self.value}"


class SymbolTable:
    def __init__(self):
        self.symbols: Dict[str, Symbol] = {}

    def define(self, name, typ):
        if name in self.symbols:
            raise Exception(f"Erro semântico: '{name}' já declarado.")
        self.symbols[name] = Symbol(name, typ)

    def lookup(self, name):
        return self.symbols.get(name)

    def assign(self, name, value):
        sym = self.lookup(name)
        if not sym:
            raise Exception(f"Erro semântico: variável '{name}' não declarada.")
        sym.value = value


class SemanticAnalyzer:
    def __init__(self):
        self.table = SymbolTable()
        self.errors: List[str] = []

    def analyze(self, ast: List[Dict[str, Any]]):
        for node in ast:
            try:
                if node["type"] == "declaration":
                    self.table.define(node["name"], node["symbol_type"])
                elif node["type"] == "assignment":
                    if not self.table.lookup(node["name"]):
                        raise Exception(f"Variável '{node['name']}' não declarada.")
                    self.table.assign(node["name"], node["value"])
                elif node["type"] == "condition":
                    cond = node["condition"]
                    left = cond["left"]
                    if self.table.lookup(left) is None:
                        raise Exception(f"Identificador '{left}' na condição não declarado.")
                    act = node["action"]
                    if not self.table.lookup(act["name"]):
                        raise Exception(f"Variável '{act['name']}' não declarada na ação.")
            except Exception as e:
                self.errors.append(str(e))
        return self.errors


# =========================================================
# GERAÇÃO DE CÓDIGO INTERMEDIÁRIO (IR)
# =========================================================
class CodeGenerator:
    def generate(self, ast: List[Dict[str, Any]]):
        ir = []
        for n in ast:
            if n["type"] == "declaration":
                ir.append({"op": "declare", "name": n["name"], "entity": n["symbol_type"]})
            elif n["type"] == "assignment":
                ir.append({"op": "assign", "target": n["name"], "value": n["value"]})
            elif n["type"] == "condition":
                ir.append({
                    "op": "if",
                    "cond": n["condition"],
                    "action": n["action"]
                })
        return ir


# =========================================================
# SIMULAÇÃO
# =========================================================
class Simulator:
    def __init__(self, symtab: SymbolTable):
        self.state = {}
        self.table = symtab

    def run(self, ir: List[Dict[str, Any]]):
        print("\n--- Simulação ---")
        for instr in ir:
            if instr["op"] == "declare":
                self.state[instr["name"]] = None
                print(f"Declarado {instr['entity']} '{instr['name']}'")
            elif instr["op"] == "assign":
                self.state[instr["target"]] = instr["value"]
                print(f"{instr['target']} <- {instr['value']}")
            elif instr["op"] == "if":
                cond = instr["cond"]
                l = self.state.get(cond["left"], 0)
                r = cond["right"]
                if isinstance(r, str) and r in self.state:
                    r = self.state[r]
                result = eval(f"{l} {cond['op']} {r}")
                print(f"if {cond['left']} {cond['op']} {cond['right']} -> {result}")
                if result:
                    act = instr["action"]
                    self.state[act["name"]] = act["value"]
                    print(f"Ação: {act['name']} <- {act['value']}")
        print("Estado final:", self.state)


# =========================================================
# EXEMPLOS
# =========================================================
def run_example(src, title):
    print(f"\n\n==== {title} ====\n")
    print(src)
    lexer = Lexer(src)
    tokens = lexer.tokenize()
    parser = Parser(tokens)
    ast = parser.parse()
    sem = SemanticAnalyzer()
    errors = sem.analyze(ast)
    if errors:
        print("Erros semânticos:")
        for e in errors:
            print("-", e)
    else:
        gen = CodeGenerator()
        ir = gen.generate(ast)
        print("IR:", json.dumps(ir, indent=2, ensure_ascii=False))
        sim = Simulator(sem.table)
        sim.run(ir)


if __name__ == "__main__":
    # Exemplo 1
    run_example("\n    sensor sensor1;\n    semaforo semA;\n    sensor1 = 5;\n    semA = 1;\n ", "Exemplo 1 - Declarações e atribuições\"")

    # Exemplo 2
    run_example("n    sensor sensA;\n    rota R1;\n    sensA = 12;\n    if sensA > 10 then R1 = 1 end\n ", "Exemplo 2 - Condição\" ")

    # Exemplo 3 (com comentário e erro semântico)
    run_example("\n    semaforo S1;\n    S2 = 1;  # S2 não declarado -> erro\n ", "Exemplo 3 - Erro semântico\" ")